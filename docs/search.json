[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01",
    "section": "",
    "text": "Code\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Projects for STA 9750 at Baruch College",
    "section": "",
    "text": "Welcome!\nHey all, my name is Lincon and I am a graduate student majoring in Quantitative Methods and Modeling.\nI am building this site throughout the semester to showcase all of the project work we are doing in STA9750.\nI am looking forward to actively updating it throughout the semester!\n\n\npackage 'sp' successfully unpacked and MD5 sums checked\npackage 'terra' successfully unpacked and MD5 sums checked\npackage 'leaflet.providers' successfully unpacked and MD5 sums checked\npackage 'png' successfully unpacked and MD5 sums checked\npackage 'raster' successfully unpacked and MD5 sums checked\npackage 'leaflet' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\linco\\AppData\\Local\\Temp\\RtmpUXzRcl\\downloaded_packages"
  },
  {
    "objectID": "mp01.html#task-1-data-acquisition",
    "href": "mp01.html#task-1-data-acquisition",
    "title": "Mini-Project #01",
    "section": "",
    "text": "Code\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}"
  },
  {
    "objectID": "mp01.html#data-import-and-preparation",
    "href": "mp01.html#data-import-and-preparation",
    "title": "Mini-Project #01",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\n\n\nCode\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8880 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)\n\n\nRows: 413620 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nstr(GLOBAL_TOP_10)\n\n\nspc_tbl_ [8,880 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ week                      : Date[1:8880], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:8880] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8880] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8880] \"KPop Demon Hunters\" \"Ruth & Boaz\" \"The Wrong Paris\" \"Man on Fire\" ...\n $ season_title              : chr [1:8880] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ weekly_hours_viewed       : num [1:8880] 32200000 15900000 13500000 15700000 11200000 8400000 6800000 6200000 4900000 8400000 ...\n $ runtime                   : num [1:8880] 1.67 1.55 1.78 2.43 1.83 ...\n $ weekly_views              : num [1:8880] 19300000 10300000 7600000 6500000 6100000 4900000 3600000 3200000 3200000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8880] 15 1 3 5 2 1 1 1 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   weekly_hours_viewed = col_double(),\n  ..   runtime = col_double(),\n  ..   weekly_views = col_double(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\nCode\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …"
  },
  {
    "objectID": "mp01.html#task-2-data-cleaning",
    "href": "mp01.html#task-2-data-cleaning",
    "title": "Mini-Project #01",
    "section": "Task 2: Data Cleaning",
    "text": "Task 2: Data Cleaning\n\n\nCode\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, show_col_types = FALSE) |&gt;\n  mutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\n\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"aka Ch…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …"
  },
  {
    "objectID": "mp01.html#task-3-data-import",
    "href": "mp01.html#task-3-data-import",
    "title": "Mini-Project #01",
    "section": "Task 3: Data Import",
    "text": "Task 3: Data Import\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na = \"N/A\", show_col_types = FALSE)\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bi…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …"
  },
  {
    "objectID": "mp01.html#initial-data-exploration",
    "href": "mp01.html#initial-data-exploration",
    "title": "Mini-Project #01",
    "section": "Initial Data Exploration",
    "text": "Initial Data Exploration\n\n\nCode\nif (!require(\"DT\")) install.packages('DT')\n\n\nLoading required package: DT\n\n\nCode\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nCode\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))"
  },
  {
    "objectID": "mp01.html#task-4-exploratory-questions",
    "href": "mp01.html#task-4-exploratory-questions",
    "title": "Mini-Project #01",
    "section": "Task 4: Exploratory Questions",
    "text": "Task 4: Exploratory Questions\nQuestion 1: How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\nCode\nn_countries &lt;- COUNTRY_TOP_10 |&gt; summarise(n = n_distinct(country_name)) |&gt; pull(n)\nn_countries\n\n\n[1] 94\n\n\nNetflix operates in 94 countries.\nQuestion 2: Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nCode\nnon_en &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(max_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(max_weeks)) |&gt;\n  slice(1)\n  non_en\n\n\n# A tibble: 1 × 2\n  show_title                     max_weeks\n  &lt;chr&gt;                              &lt;dbl&gt;\n1 All Quiet on the Western Front        23\n\n\nAmong non-English-language films, All Quiet on the Western Front holds the longest global Top-10 presence with 23 weeks cumulatively\nQuestion 3: What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(category, \"^Films\")) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  group_by(show_title) |&gt;\n  summarize(longest = max(runtime_minutes, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(longest)) |&gt;\n  slice(1)\n\n\nWarning: There were 769 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `longest = max(runtime_minutes, na.rm = TRUE)`.\nℹ In group 2: `show_title = \"'83\"`.\nCaused by warning in `max()`:\n! no non-missing arguments to max; returning -Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 768 remaining warnings.\n\n\nCode\nlongest_film_title   &lt;- longest_film$show_title\nlongest_film_minutes &lt;- longest_film$longest\nlongest_film_title\n\n\n[1] \"Pushpa 2: The Rule (Reloaded Version)\"\n\n\nCode\nlongest_film_minutes\n\n\n[1] 224\n\n\nThe longest film to chart globally is Pushpa 2: The Rule (Reloaded Version) at 224 minutes\nQuestion 4:For each of the four categories, what program has the most total hours of global viewership?\n\n\nCode\ntop_hours &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups=\"drop\") |&gt;\n  group_by(category) |&gt; slice_max(total_hours, n=1, with_ties=FALSE)\n\ntop_hours\n\n\n# A tibble: 4 × 3\n# Groups:   category [4]\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    591300000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000\n\n\nThe leading program for each category is Films (English): KPop Demon Hunters; Films (Non-English): Society of the Snow; TV (English): Stranger Things; TV (Non-English): Squid Game\nQuestion 5: Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nCode\nlongest_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"^TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarize(run_weeks = n_distinct(week), .groups = \"drop\") |&gt;\n  arrange(desc(run_weeks)) |&gt;\n  slice(1)\nlongest_run\n\n\n# A tibble: 1 × 3\n  country_name show_title  run_weeks\n  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;\n1 Pakistan     Money Heist       128\n\n\nMoney Heist ran 128 weeks in Pakistan.\nQuestion 6: Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nCode\nno_hist &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarize(n_weeks = n_distinct(week), last_week = max(week), .groups = \"drop\") |&gt;\n  arrange(n_weeks)\n\nhead(no_hist, 5) |&gt;\n  format_titles() |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nCode\nno_hist\n\n\n# A tibble: 94 × 3\n   country_name n_weeks last_week \n   &lt;chr&gt;          &lt;int&gt; &lt;date&gt;    \n 1 Russia            35 2022-02-27\n 2 Argentina        222 2025-09-28\n 3 Australia        222 2025-09-28\n 4 Austria          222 2025-09-28\n 5 Bahamas          222 2025-09-28\n 6 Bahrain          222 2025-09-28\n 7 Bangladesh       222 2025-09-28\n 8 Belgium          222 2025-09-28\n 9 Bolivia          222 2025-09-28\n10 Brazil           222 2025-09-28\n# ℹ 84 more rows\n\n\nThe smallest-history country is Russia, last reporting 2022-02-27, with 35 weeks of data.\nQuestion 7: What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nCode\noptions(scipen = 999)  \n\ntotal_hours &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, fixed(\"Squid Game\", ignore_case = TRUE))) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt;\n  pull(total_hours)\n\ntotal_hours  \n\n\n[1] 5310000000\n\n\nSquid Game had 5310000000 hours of global view time across all seasons.\nQuestion 8: The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021?\n\n\nCode\nred_runtime_hours &lt;- 1 + 58/60   \nhours &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt;\n  pull(total_hours)\n\nviews &lt;- hours / red_runtime_hours\n\nviews\n\n\n[1] 201732203\n\n\nThere are 201,732,203 views for Red Notice in 2021.\nQuestion 9: How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\nCode\nfilms &lt;- COUNTRY_TOP_10 |&gt; filter(category == \"Films\")\nus_films &lt;- films |&gt; filter(country_name == \"United States\")\n\never_us_no1 &lt;- us_films |&gt;\n  group_by(show_title) |&gt;\n  summarise(ever_us_no1 = any(weekly_rank == 1, na.rm = TRUE), .groups = \"drop\")\n\nus_most_recent_1 &lt;- us_films |&gt;\n  filter(weekly_rank == 1) |&gt;\n  group_by(show_title) |&gt;\n  summarise(us_most_recent_week_at_1 = max(week, na.rm = TRUE), .groups = \"drop\")\n\nus_first_week &lt;- us_films |&gt;\n  group_by(show_title) |&gt;\n  summarise(us_first_week = min(week, na.rm = TRUE), .groups = \"drop\")\n\ndebut_week &lt;- films |&gt;\n  group_by(show_title) |&gt;\n  summarise(debut_week = min(week, na.rm = TRUE), .groups = \"drop\")\n\ndebut_countries &lt;- films |&gt;\n  inner_join(debut_week, by = \"show_title\") |&gt;\n  filter(week == debut_week) |&gt;\n  group_by(show_title) |&gt;\n  summarise(debut_countries = list(sort(unique(country_name))), .groups = \"drop\") |&gt;\n  mutate(debut_in_us = purrr::map_lgl(debut_countries, ~ \"United States\" %in% .x))\n\nlibrary(purrr)\n\nsummary_tbl &lt;- ever_us_no1 |&gt;\n  filter(ever_us_no1) |&gt;\n  left_join(us_most_recent_1, by = \"show_title\") |&gt;\n  left_join(us_first_week,     by = \"show_title\") |&gt;\n  left_join(debut_week,        by = \"show_title\") |&gt;\n  left_join(debut_countries,   by = \"show_title\") |&gt;\n  filter(!debut_in_us) |&gt;\n  arrange(desc(us_most_recent_week_at_1))\n\ncount  &lt;- nrow(summary_tbl)\nrecent &lt;- if (count &gt; 0) summary_tbl$show_title[1] else NA_character_\nrecent_date &lt;- if (count &gt; 0) as.character(summary_tbl$us_most_recent_week_at_1[1]) else NA_character_\n\nhead(summary_tbl, 40) |&gt;\n  transmute(\n    Title = show_title,\n    `Global Debut Week` = debut_week,\n    `Debut Countries` = sapply(debut_countries, paste, collapse = \", \"),\n    `First Week in US` = us_first_week,\n    `Most Recent US #1 Week` = us_most_recent_week_at_1\n  )\n\n\n# A tibble: 33 × 5\n   Title                `Global Debut Week` `Debut Countries` `First Week in US`\n   &lt;chr&gt;                &lt;date&gt;              &lt;chr&gt;             &lt;date&gt;            \n 1 Plane                2024-09-15          Ireland, Malta, … 2025-06-15        \n 2 The Wild Robot       2025-04-20          Australia         2025-05-25        \n 3 Despicable Me 4      2025-01-19          Australia, New Z… 2025-03-02        \n 4 Venom: The Last Dan… 2025-01-26          Bangladesh, Indi… 2025-03-02        \n 5 To Catch a Killer    2023-08-20          Canada            2025-02-23        \n 6 Despicable Me 2      2021-10-10          Canada            2022-02-06        \n 7 Focus                2021-07-04          Latvia            2024-11-17        \n 8 Bad Boys: Ride or D… 2024-09-08          Bangladesh, Indi… 2024-10-13        \n 9 The Garfield Movie   2024-08-25          Pakistan          2024-09-22        \n10 Jack Reacher: Never… 2021-10-03          Denmark, Finland… 2024-08-04        \n# ℹ 23 more rows\n# ℹ 1 more variable: `Most Recent US #1 Week` &lt;date&gt;\n\n\n33 films debuted outside the US but later reached #1 in the US; the most recent is Plane (US #1 week: 2025-06-22).\nQuestion 10: Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nCode\ndebut &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(show_title, season_title) |&gt;\n  summarise(debut_week = min(week), .groups = \"drop\")\n\n\nspread &lt;- COUNTRY_TOP_10 |&gt;\n  inner_join(debut, by = c(\"show_title\",\"season_title\")) |&gt;\n  filter(week == debut_week, str_detect(category, \"^TV\")) |&gt;\n  group_by(show_title, season_title) |&gt;\n  summarise(countries = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries)) |&gt;\n  slice(1)\n\ntitle &lt;- paste0(spread$show_title, ifelse(is.na(spread$season_title),\"\", paste0(\" — \", spread$season_title)))\nn     &lt;- spread$countries[1]\n\ntitle\n\n\n[1] \"Emily in Paris — Emily in Paris: Season 2\"\n\n\nCode\nn\n\n\n[1] 94\n\n\nIn its debut week, Emily in Paris — Emily in Paris: Season 2 charted in 94 countries.\n##Task 5: Stranger Things Press Release\nStranger Things; Whats Next!?\n\n\nCode\nst_global &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(week = as.Date(week)) %&gt;%\n  filter(str_detect(category, \"^TV\"),\n         str_detect(show_title, fixed(\"Stranger Things\", ignore_case = TRUE)))\n\nst_total_hours  &lt;- sum(st_global$weekly_hours_viewed, na.rm = TRUE)\nst_weeks_global &lt;- dplyr::n_distinct(st_global$week)\n\n\nst_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(week = as.Date(week)) %&gt;%\n  filter(str_detect(show_title, fixed(\"Stranger Things\", ignore_case = TRUE))) %&gt;%\n  summarise(n = dplyr::n_distinct(country_name)) %&gt;%\n  pull(n)\n\n\ntv_hours &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"^TV\")) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(total_hours))\n\nst_rows &lt;- which(str_detect(tv_hours$show_title, fixed(\"Stranger Things\", ignore_case = TRUE)))\nst_rank &lt;- if (length(st_rows)) st_rows[1] else NA_integer_\n\n\npeer_tv &lt;- head(tv_hours$show_title, 3)\n\n\ntv_hours_en &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(category == \"TV (English)\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(total_hours))\n\nst_rows_en &lt;- which(str_detect(tv_hours_en$show_title, fixed(\"Stranger Things\", ignore_case = TRUE)))\nst_rank_en &lt;- if (length(st_rows_en)) st_rows_en[1] else NA_integer_\nst_global\n\n\n# A tibble: 50 × 9\n   week       category   weekly_rank show_title season_title weekly_hours_viewed\n   &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;\n 1 2022-10-09 TV (Engli…           9 Stranger … Stranger Th…             9420000\n 2 2022-10-02 TV (Engli…           8 Stranger … Stranger Th…            10340000\n 3 2022-09-18 TV (Engli…           9 Stranger … Stranger Th…            13480000\n 4 2022-09-11 TV (Engli…           8 Stranger … Stranger Th…            16560000\n 5 2022-09-04 TV (Engli…           5 Stranger … Stranger Th…            20280000\n 6 2022-08-28 TV (Engli…           4 Stranger … Stranger Th…            23640000\n 7 2022-08-21 TV (Engli…           3 Stranger … Stranger Th…            28800000\n 8 2022-08-14 TV (Engli…           4 Stranger … Stranger Th…            35310000\n 9 2022-08-07 TV (Engli…           4 Stranger … Stranger Th…            44760000\n10 2022-08-07 TV (Engli…          10 Stranger … Stranger Th…            17040000\n# ℹ 40 more rows\n# ℹ 3 more variables: runtime &lt;dbl&gt;, weekly_views &lt;dbl&gt;,\n#   cumulative_weeks_in_top_10 &lt;dbl&gt;\n\n\nCode\nst_total_hours\n\n\n[1] 2967980000\n\n\nCode\nst_countries\n\n\n[1] 93\n\n\nCode\nst_weeks_global\n\n\n[1] 20\n\n\nCode\nst_rank\n\n\n[1] 2\n\n\nCode\npeer_tv \n\n\n[1] \"Squid Game\"      \"Stranger Things\" \"Wednesday\"      \n\n\nStranger Things season 5 will debut later this year in 2025. The previous four seasons of the show have lined up season 5 perfectly. The last four seasons have accumulated 2967980000 of viewership and was Top 10 in 93 countries. These are very impressive stats! This shows that the show has proven to be popular across many countries acquiring and enormous amount of views. The previous four seasons also spent 20 weeks in the top ten position globally and ranks 2 currently among the top three titles Squid Game, Stranger Things, Wednesday. Because the last four seasons have sat with other top titles and has relatively retained a strong rank among them, season 5 is sure to not disappoint. So everyone get ready, turn on your tv, lets head to Netflix and enjoy the show!!\n##Task 6: Netflix in India Press Release\nNetflix Hits Bollywood!\n\n\nCode\nINDIA &lt;- COUNTRY_TOP_10 |&gt; filter(country_name == \"India\")\nUSA   &lt;- COUNTRY_TOP_10 |&gt; filter(country_name == \"United States\")\n\nindia_first_titles &lt;- INDIA |&gt; distinct(show_title) |&gt;\n  anti_join(USA |&gt; distinct(show_title), by = \"show_title\")\n\nviews_base &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title %in% india_first_titles$show_title) |&gt;\n  mutate(\n    year         = year(week),\n    runtime      = suppressWarnings(as.numeric(runtime)),       \n    weekly_views = suppressWarnings(as.numeric(weekly_views)),  \n    views_est    = dplyr::coalesce(\n      weekly_views,\n      if_else(!is.na(runtime) & runtime &gt; 0, weekly_hours_viewed / runtime, NA_real_)\n    )\n  )\n\nlatest_india_week &lt;- INDIA |&gt;\n  filter(show_title %in% india_first_titles$show_title) |&gt;\n  group_by(show_title) |&gt;\n  summarise(india_latest_week = max(week, na.rm = TRUE), .groups = \"drop\")\n\nviews_per_title &lt;- views_base |&gt;\n  group_by(show_title) |&gt;\n  summarise(all_time_views = sum(views_est, na.rm = TRUE), .groups = \"drop\")\n\nrecent_top3 &lt;- latest_india_week |&gt;\n  inner_join(views_per_title, by = \"show_title\") |&gt;\n  arrange(desc(india_latest_week)) |&gt;\n  slice_head(n = 3)\n\nt1  &lt;- if (nrow(recent_top3) &gt;= 1) recent_top3$show_title[1] else NA_character_\nt1v &lt;- if (nrow(recent_top3) &gt;= 1) recent_top3$all_time_views[1] else NA_real_\nt2  &lt;- if (nrow(recent_top3) &gt;= 2) recent_top3$show_title[2] else NA_character_\nt2v &lt;- if (nrow(recent_top3) &gt;= 2) recent_top3$all_time_views[2] else NA_real_\nt3  &lt;- if (nrow(recent_top3) &gt;= 3) recent_top3$show_title[3] else NA_character_\nt3v &lt;- if (nrow(recent_top3) &gt;= 3) recent_top3$all_time_views[3] else NA_real_\n\n\ntotal_views_all_time &lt;- sum(views_per_title$all_time_views, na.rm = TRUE)\n\n\nstart_year &lt;- min(views_base$year, na.rm = TRUE)\nfirst3     &lt;- start_year + 0:2\n\nviews_first3_years &lt;- views_base |&gt;\n  filter(year %in% first3) |&gt;\n  summarise(total_views = sum(views_est, na.rm = TRUE), .groups = \"drop\") |&gt;\n  pull(total_views)\n\nshare_first3 &lt;- ifelse(total_views_all_time &gt; 0,\n                       views_first3_years / total_views_all_time, NA_real_)\n\nwindow_start_all &lt;- min(GLOBAL_TOP_10$week, na.rm = TRUE)\nwindow_end_all   &lt;- max(GLOBAL_TOP_10$week, na.rm = TRUE)\nt1\n\n\n[1] \"Bon Appétit, Your Majesty\"\n\n\nCode\nscales::comma(round(t1v))\n\n\n[1] \"38,400,000\"\n\n\nCode\nscales::comma(round(total_views_all_time))\n\n\n[1] \"2,742,900,000\"\n\n\nCode\nscales::comma(round(views_first3_years))\n\n\n[1] \"624,500,000\"\n\n\nCode\nifelse(is.na(share_first3), \"N/A\", paste0(round(100*share_first3), \"%\"))\n\n\n[1] \"23%\"\n\n\nNetflix, who doesnt know that name around the globe? Netflix has recently broke into the Indian market and its creating a lot of buzz. So far the total views all time in India have been 2,742,900,000. This shows that there is strong viewership in India and that Netflix has a lot of gain coming from this market. Some of the top recent successes are Bon Appétit, Your Majesty (38,400,000 views), Dhadak 2 (1,300,000 views), and Inspector Zende (7,800,000 views), reflecting strong momentum within the nation! 624,500,000 out of those all-time views (23%) arrived in the first three years. Thus showing us long term growth stands strong within India. The Indian people are watching and are waiting for more!\n##Task 7: 3rd Press Release\nCurrent Best For Netflix\n\n\nCode\nGLOBAL_TOP_10  &lt;- GLOBAL_TOP_10  %&gt;%\n  mutate(\n    week         = as.Date(week),\n    runtime      = suppressWarnings(as.numeric(runtime)),\n    weekly_views = suppressWarnings(as.numeric(weekly_views)),\n    views_est    = dplyr::coalesce(weekly_views,\n                                   if_else(!is.na(runtime) & runtime &gt; 0,\n                                           weekly_hours_viewed / runtime, NA_real_))\n  )\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(week = as.Date(week))\n\n\nusa_films_titles &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"United States\", category == \"Films\") %&gt;%\n  distinct(show_title)\n\nnon_us_film_titles &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(category == \"Films\") %&gt;%\n  distinct(show_title) %&gt;%\n  anti_join(usa_films_titles, by = \"show_title\")\n\n\nfilm_views &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"^Films\"),\n         show_title %in% non_us_film_titles$show_title) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_views_est = sum(views_est, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(total_views_est))\n\nbest_film_title &lt;- if (nrow(film_views)) film_views$show_title[1] else NA_character_\nbest_film_views &lt;- if (nrow(film_views)) film_views$total_views_est[1] else NA_real_\n\nall_countries &lt;- COUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% arrange(country_name) %&gt;% pull(country_name)\n\nshown_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(show_title == best_film_title, category == \"Films\") %&gt;%\n  distinct(country_name) %&gt;%\n  arrange(country_name) %&gt;%\n  pull(country_name)\n\nnot_shown_countries &lt;- setdiff(all_countries, shown_countries)\n\nn_shown     &lt;- length(shown_countries)\nn_not_shown &lt;- length(not_shown_countries)\n\nbest_film_title\n\n\n[1] \"Dr. Seuss' The Grinch\"\n\n\nCode\nbest_film_views\n\n\n[1] 81200000\n\n\nCode\nn_shown\n\n\n[1] 84\n\n\nCode\nn_not_shown\n\n\n[1] 10\n\n\nWhat is the next best for Netflix? Have you ever wondered what titles were grossing outside of the us with a lot of viewer count? Believe it or not it seems to be Dr. Seuss’ The Grinch Even though this seems to be a film that has come out many times with remakes, it still is very successful. With 81200000 views outside of the US, it shows strong precense globally. Currently it is being shown in 84 countries and not being shown in 10 countries. For this holiday season Netflix should target this movie in all countries possible. This would drive more global and customer expansion with a title that has already proven to be successful!!"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "",
    "text": "Code\n.indent-block-tab {\ntext-indent: 2em;    \n}"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Introduction",
    "text": "Introduction\n\nHousing costs are one of the biggest pressures facing U.S. cities. In this mini-project I’m exploring where housing is most (and least) affordable and which metro areas are actually building enough new homes to keep up with demand."
  },
  {
    "objectID": "mp02.html#question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "href": "mp02.html#question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "title": "Mini-Project #02 — Making Backyards Affordable for All",
    "section": "Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?",
    "text": "Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\npermits_10s &lt;- PERMITS %&gt;%\n  # Standardize column names and CBSA id r\n  transmute(cbsa5 = as.integer(CBSA),\n            year,\n            units = new_housing_units_permitted) %&gt;%\n  # Restrict to the 2010–2019 window the question asks about\n  filter(year &gt;= 2010, year &lt;= 2019)\n\nq1 &lt;- permits_10s %&gt;%\n  # Add up all annual permits within the decade for each CBSA\n  group_by(cbsa5) %&gt;%\n  summarise(permits_total = sum(units, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  # Attach metro names for readability in the written answer\n  left_join(cbsa_names, by = \"cbsa5\") %&gt;%\n  # Rank from most → least permits and keep the top row = answer\n  arrange(desc(permits_total)) %&gt;%\n  slice(1)\n\n\n\nFrom 2010–2019, the most new housing units were permitted in Houston-Sugar Land-Baytown, TX Metro Area with 482,075 permits."
  },
  {
    "objectID": "mp02.html#question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "href": "mp02.html#question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "title": "Mini-Project #02 — Making Backyards Affordable for All",
    "section": "Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?",
    "text": "Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nq2 &lt;- PERMITS %&gt;%\n  transmute(\n    cbsa5 = as.integer(CBSA),                    # normalize key\n    year,\n    units = new_housing_units_permitted\n  ) %&gt;%\n  filter(cbsa5 == 10740) %&gt;%                     # Albuquerque CBSA\n  arrange(desc(units)) %&gt;%                       # rank years by units\n  slice(1)                                       # take the max year\n\n\n\nIn the year 2021 Albuquerque, NM permitted the most new housing units at 4,021."
  },
  {
    "objectID": "mp02.html#task-1-data-acquisition",
    "href": "mp02.html#task-1-data-acquisition",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 1: Data acquisition",
    "text": "Task 1: Data acquisition\n\nHere we will incorporate the instructor provided code into our report. This will help with the data acquisition.\n\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(DT)\nlibrary(gt)\nlibrary(stringr)\nlibrary(ggrepel)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    \n    if(!file.exists(fname)){\n    \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code)\n    \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n    \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n# Standardized CBSA keys\nas_cbsa5_census &lt;- function(x) x %&gt;% mutate(cbsa5 = as.integer(GEOID))\n# BLS \"C1234\"  -&gt; Census \"12340\"\nas_cbsa5_bls &lt;- function(x){\n  x %&gt;% mutate(cbsa5 = as.integer(paste0(readr::parse_number(FIPS), \"0\")))\n}\n\n\n# CBSA name lookup \ncbsa_names &lt;- INCOME %&gt;%\n  as_cbsa5_census() %&gt;%\n  select(cbsa5, NAME) %&gt;% distinct()\n\n\n\nRelationship Diagram\n\n\nCode\nlibrary(DiagrammeR)\n\nerd_dot &lt;- \"\ndigraph ERD {\n  graph [rankdir=LR, fontsize=10, labelloc=t, label='Mini-Project 02 — Data ERD'];\n  node  [shape=plaintext, fontname='Helvetica'];\n  edge  [fontsize=9, color='#555555'];\n\n  INCOME [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;INCOME (ACS)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;GEOID&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;year&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;household_income&lt;/TD&gt;&lt;TD&gt;number&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;NAME&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  RENT [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;RENT (ACS)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;GEOID&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;year&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;monthly_rent&lt;/TD&gt;&lt;TD&gt;number&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  POP [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;POPULATION (ACS)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;GEOID&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;year&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;population&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  HH [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;HOUSEHOLDS (ACS)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;GEOID&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;year&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;households&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  PERMITS [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;PERMITS (Census BPS)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;CBSA&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;year&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;new_housing_units_permitted&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  WAGES [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;WAGES (BLS QCEW)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;FIPS&lt;/TD&gt;&lt;TD&gt;string (e.g., C1234)&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;INDUSTRY&lt;/TD&gt;&lt;TD&gt;int (NAICS)&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;EMPLOYMENT&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;TOTAL_WAGES&lt;/TD&gt;&lt;TD&gt;number&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;AVG_WAGE&lt;/TD&gt;&lt;TD&gt;number&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;YEAR&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  ICODES [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;INDUSTRY_CODES (NAICS titles)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;level1_code..level4_code&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;level1_title..level4_title&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  NAMES [label=&lt;\n    &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n      &lt;TR&gt;&lt;TD BGCOLOR='lightgray' COLSPAN='2'&gt;&lt;B&gt;CBSANAMES (lookup)&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;cbsa5&lt;/TD&gt;&lt;TD&gt;int&lt;/TD&gt;&lt;/TR&gt;\n      &lt;TR&gt;&lt;TD ALIGN='left'&gt;NAME&lt;/TD&gt;&lt;TD&gt;string&lt;/TD&gt;&lt;/TR&gt;\n    &lt;/TABLE&gt;\n  &gt;];\n\n  // Intra-ACS joins (same grain: GEOID + year)\n  INCOME -&gt; RENT [label='GEOID + year', color='#777777', arrowsize=0.7];\n  INCOME -&gt; POP  [label='GEOID + year', color='#777777', arrowsize=0.7];\n  INCOME -&gt; HH   [label='GEOID + year', color='#777777', arrowsize=0.7];\n\n  // Map ACS to standardized CBSA key\n  INCOME -&gt; NAMES [label='as.integer(GEOID) = cbsa5', arrowsize=0.7];\n\n  // External sources mapped via cbsa5\n  PERMITS -&gt; NAMES [label='CBSA = cbsa5', arrowsize=0.7];\n  WAGES   -&gt; NAMES [label='FIPS C1234 → cbsa5 12340', arrowsize=0.7];\n\n  // NAICS titles for BLS industries\n  WAGES -&gt; ICODES [label='INDUSTRY (NAICS)', arrowsize=0.7];\n}\n\"\n\nDiagrammeR::grViz(erd_dot)"
  },
  {
    "objectID": "mp02.html#task-2-multi-table-questions",
    "href": "mp02.html#task-2-multi-table-questions",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 2: Multi-Table Questions",
    "text": "Task 2: Multi-Table Questions\n\nHere we will look at the data and answer some exploratory questions which will inspire some later thoughts.\n\n\nQuestion 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\npermits_10s &lt;- PERMITS %&gt;%\n  # Standardize column names and CBSA id \n  transmute(cbsa5 = as.integer(CBSA),\n            year,\n            units = new_housing_units_permitted) %&gt;%\n  # Restrict to the 2010–2019 window \n  filter(year &gt;= 2010, year &lt;= 2019)\n\nq1 &lt;- permits_10s %&gt;%\n  # Add up all annual permits within the decade for each CBSA\n  group_by(cbsa5) %&gt;%\n  summarise(permits_total = sum(units, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  # Attach metro names \n  left_join(cbsa_names, by = \"cbsa5\") %&gt;%\n  # Rank from most → least permits \n  arrange(desc(permits_total)) %&gt;%\n  slice(1)\n\n\n\nFrom 2010–2019, the most new housing units were permitted in Houston-Sugar Land-Baytown, TX Metro Area with 482,075 permits.\n\n\n\nQuestion 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nq2 &lt;- PERMITS %&gt;%\n  transmute(\n    cbsa5 = as.integer(CBSA),                    \n    year,\n    units = new_housing_units_permitted\n  ) %&gt;%\n  filter(cbsa5 == 10740) %&gt;%                     # Albuquerque CBSA\n  arrange(desc(units)) %&gt;%                       # rank years by units\n  slice(1)                                       # take the max year\n\n\n\nIn the year 2021 Albuquerque, NM permitted the most new housing units at 4,021.\n\n\n\nQuestion 3: Which state (not CBSA) had the highest average individual income in 2015?\n\n\nCode\n# Pull 2015 median household income from ACS \ninc15 &lt;- INCOME %&gt;%\n  filter(year == 2015) %&gt;%\n  as_cbsa5_census() %&gt;%\n  transmute(\n    cbsa5,\n    NAME,                         # carries \", ST\" for state extraction\n    hh_inc = household_income              \n  )\n\n# Pull 2015 households \nhh15 &lt;- HOUSEHOLDS %&gt;%\n  filter(year == 2015) %&gt;%\n  as_cbsa5_census() %&gt;%\n  transmute(\n    cbsa5,\n    households                              \n  )\n\n# Pull 2015 population \npop15 &lt;- POPULATION %&gt;%\n  filter(year == 2015) %&gt;%\n  as_cbsa5_census() %&gt;%\n  transmute(\n    cbsa5,\n    population                              \n  )\n\n# Join at CBSA level and compute per-state average individual income\nq3 &lt;- inc15 %&gt;%\n  inner_join(hh15,  by = \"cbsa5\") %&gt;%      # need BOTH income & households\n  inner_join(pop15, by = \"cbsa5\") %&gt;%      # need population to compute per-person\n  mutate(\n    total_income = hh_inc * households,    \n    state_abbr = stringr::str_match(NAME, \",\\\\s*([A-Z]{2})\")[, 2]\n  ) %&gt;%\n  group_by(state_abbr) %&gt;%\n  summarise(\n    state_total_income = sum(total_income, na.rm = TRUE),\n    state_population   = sum(population,   na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(avg_indiv_income = state_total_income / state_population) %&gt;%\n  arrange(desc(avg_indiv_income)) %&gt;%\n  slice(1)                                  \n\n\n\nIn 2015, the state with the highest average individual income was DC.\n\n\n\nQuestion 4: Data scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists.\n\n\nCode\n# Keep every CBSA that ties for the maximum per year\nw5182_winners &lt;- WAGES %&gt;%\n  as_cbsa5_bls() %&gt;%\n  filter(INDUSTRY == 5182) %&gt;%\n  group_by(YEAR) %&gt;%\n  filter(EMPLOYMENT == max(EMPLOYMENT, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  left_join(cbsa_names, by = \"cbsa5\")\n\n# NYC last winning year \nnyc_rows &lt;- w5182_winners %&gt;% filter(grepl(\"^New York\", NAME))\nnyc_last_year &lt;- if (nrow(nyc_rows) == 0) NA_integer_ else max(nyc_rows$YEAR)\n\nnyc_last_year_str &lt;- if (is.na(nyc_last_year)) \"never (2009–2023, excl. 2020)\" else as.character(nyc_last_year)\n\n\n\nNew York City last led the nation in Data Science and Business Analyst employment in 2015.\n\n\n\nQuestion 5: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\nq5_fin_share &lt;- WAGES %&gt;%\n  as_cbsa5_bls() %&gt;%                                      \n  mutate(ind_chr = as.character(INDUSTRY),\n         ind_len = nchar(ind_chr)) %&gt;%\n  filter(ind_len == 2) %&gt;%                                \n  group_by(cbsa5, YEAR) %&gt;%\n  summarise(\n    total_wages_2d = sum(TOTAL_WAGES, na.rm = TRUE),      # total *sector* wages in the CBSA-year\n    finance_wages  = sum(TOTAL_WAGES[ind_chr == \"52\"],    # wages in Finance & Insurance (NAICS 52)\n                         na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(finance_share = finance_wages / total_wages_2d) %&gt;%  # share of wages in finance\n   left_join(cbsa_names, by = \"cbsa5\")                 \n\n# Identify the numeric CBSA id for New York metro\nnyc_cbsa5 &lt;- cbsa_names %&gt;%\n  filter(stringr::str_detect(NAME, \"^New York\")) %&gt;%\n  slice(1) %&gt;% pull(cbsa5)\n\n# Select NYC rows, then take the year with the highest finance_share\nq5_nyc &lt;- q5_fin_share %&gt;%\n  filter(cbsa5 == nyc_cbsa5) %&gt;%\n  arrange(desc(finance_share)) %&gt;%\n  slice(1)\n\n\n\nIn 2014, Finance & Insurance reached its peak wage share in NYC at 33%."
  },
  {
    "objectID": "mp02.html#task-3-initial-visualizations",
    "href": "mp02.html#task-3-initial-visualizations",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 3: Initial Visualizations",
    "text": "Task 3: Initial Visualizations\n\nUsing the functions of the ggplot2 package, we will create suitable visualizations for each of the following relationships.\n\n\nRelationship 1: The relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\nrent_income_2009 &lt;- INCOME %&gt;%\n  filter(year == 2009) %&gt;%\n  select(GEOID, NAME, household_income) %&gt;%\n  inner_join(RENT %&gt;% filter(year == 2009) %&gt;% select(GEOID, monthly_rent),\n             by = \"GEOID\") %&gt;%\n  mutate(cbsa5 = as.integer(GEOID))\nggplot(\n  rent_income_2009,\n  aes(x = household_income,\n      y = monthly_rent,\n      color = (monthly_rent * 12) / household_income)  # rent-to-income ratio\n) +\n  geom_point(size = 2.2, alpha = 0.85) +\n  scale_color_viridis_c(\n    option = \"RdBu\",                 # high-contrast palette\n    end = 0.95,\n    name = \"Rent / Income\",\n    labels = scales::percent_format(accuracy = 1)\n  ) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"2009 Monthly Rent vs. Household Income\",\n    x = \"Median Household Income (annual)\",\n    y = \"Median Gross Rent (monthly)\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    panel.grid.major = element_line(linewidth = 0.3),\n    panel.grid.minor = element_line(linewidth = 0.2)\n  )\n\n\n\n\n\n\n\n\n\n\nIn 2009, metros with higher household income generally had higher monthly rent, and the colors suggest rent usually ran about 20–30% of income.\n\n\n\nRelationship 2: The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs. Design your visualization so that it is possible to see the evolution of this relationship over time.\n\n\nCode\nif (!exists(\"emp_totals_2d\")) {\n  emp_totals_2d &lt;- WAGES %&gt;%\n    as_cbsa5_bls() %&gt;%                                   \n    mutate(ind = as.character(INDUSTRY),\n           ind_len = nchar(ind)) %&gt;%\n    filter(ind_len == 2) %&gt;%                              \n    group_by(cbsa5, YEAR) %&gt;%\n    summarise(\n      total_emp_2d = sum(EMPLOYMENT, na.rm = TRUE),              \n      hc_emp       = sum(EMPLOYMENT[ind == \"62\"], na.rm = TRUE), \n      .groups = \"drop\"\n    ) %&gt;%\n    left_join(cbsa_names, by = \"cbsa5\")\n}\n\n# readable set of metros (top 40 by scale)\nif (!exists(\"top40\")) {\n  top40 &lt;- emp_totals_2d %&gt;%\n    group_by(cbsa5) %&gt;%\n    summarise(max_emp = max(total_emp_2d, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    slice_max(max_emp, n = 40)\n}\n\n#Compute HC share  \nemp_smallmult &lt;- emp_totals_2d %&gt;%\n  mutate(hc_share = hc_emp / total_emp_2d) %&gt;%\n  semi_join(top40, by = \"cbsa5\")\n\n#Per-year metrics & label: correlation (r) and OLS slope (β)\n\nr_by_year &lt;- emp_smallmult %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    r     = cor(total_emp_2d, hc_emp, use = \"complete.obs\"),\n    sd_x  = sd(total_emp_2d, na.rm = TRUE),\n    sd_y  = sd(hc_emp,       na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    beta  = r * (sd_y / sd_x),\n    x_pos = max(emp_smallmult$total_emp_2d, na.rm = TRUE) * 0.95,\n    y_pos = max(emp_smallmult$hc_emp,       na.rm = TRUE) * 0.95,\n    lab   = paste0(\"r = \", round(r, 2), \" · β = \", round(beta, 3))\n  )\n\n# Plot: one panel per year; color = HC share; line = per-year linear fit\nggplot(emp_smallmult, aes(x = total_emp_2d, y = hc_emp, color = hc_share)) +\n  geom_point(size = 1.6, alpha = 0.85) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.7, color = \"grey35\") +\n  facet_wrap(~ YEAR, ncol = 4) +\n  scale_color_viridis_c(\n    name = \"HC share of jobs\",\n    labels = scales::label_percent(accuracy = 1),\n    option = \"plasma\", end = 0.95\n  ) +\n  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  geom_text(data = r_by_year,\n            aes(x = x_pos, y = y_pos, label = lab),\n            inherit.aes = FALSE, size = 3, hjust = 1, vjust = 1, color = \"grey20\") +\n  labs(\n    title = \"Health Care Employment vs. Total Employment\",\n    x = \"Total employment\",\n    y = \"Health Care Employment)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\nHealth care employment scales closely with total metro jobs, and in recent years the points suggests the sector is taking a bigger, steadier share of city employment over time.\n\n\n\nRelationship 3: The evolution of average household size over time.\n\n\nCode\nnyc_cbsa5 &lt;- cbsa_names %&gt;% filter(stringr::str_detect(NAME, \"^New York\")) %&gt;% slice(1) %&gt;% pull(cbsa5)\nla_cbsa5  &lt;- cbsa_names %&gt;% filter(stringr::str_detect(NAME, \"^Los Angeles\")) %&gt;% slice(1) %&gt;% pull(cbsa5)\n\n# Build series and tag highlights\nHHSIZE &lt;- POPULATION %&gt;%\n  as_cbsa5_census() %&gt;%\n  select(cbsa5, year, population) %&gt;%\n  inner_join(HOUSEHOLDS %&gt;% as_cbsa5_census() %&gt;% select(cbsa5, year, households),\n             by = c(\"cbsa5\",\"year\")) %&gt;%\n  mutate(hh_size = population / households) %&gt;%\n  left_join(cbsa_names, by = \"cbsa5\") %&gt;%\n  mutate(\n    is_highlight    = cbsa5 %in% c(nyc_cbsa5, la_cbsa5),\n    highlight_label = dplyr::case_when(\n      cbsa5 == nyc_cbsa5 ~ \"NYC\",\n      cbsa5 == la_cbsa5  ~ \"LA\",\n      TRUE               ~ NA_character_\n    )\n  )\n\n# Label positions (latest year) for the two highlights only\nlabels_df &lt;- HHSIZE %&gt;%\n  filter(is_highlight) %&gt;%\n  group_by(cbsa5) %&gt;%\n  filter(year == max(year, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nggplot() +\n  # Others in light gray\n  geom_line(data = dplyr::filter(HHSIZE, !is_highlight),\n            aes(x = year, y = hh_size, group = cbsa5),\n            color = \"grey78\", linewidth = 0.4, alpha = 0.45) +\n  # NYC & LA in fixed colors\n  geom_line(data = dplyr::filter(HHSIZE, is_highlight),\n            aes(x = year, y = hh_size, color = highlight_label, group = cbsa5),\n            linewidth = 1.6) +\n  # Direct labels colored to match lines\n  ggrepel::geom_text_repel(\n    data = labels_df,\n    aes(x = year, y = hh_size, label = NAME, color = highlight_label),\n    nudge_x = 0.45, direction = \"y\", hjust = 0, segment.alpha = 0.4, size = 3\n  ) +\n  scale_color_manual(values = c(NYC = \"#D81B60\", LA = \"#1E88E5\"), guide = \"none\") +\n  scale_x_continuous(expand = expansion(mult = c(0.02, 0.16))) +\n  labs(\n    title    = \"Average Household Size over Time by CBSA\",\n    subtitle = \"NYC and LA \",\n    x = \"Year\", y = \"Average household size\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\nMost metros barely move over time, while NYC and LA sit on the higher end of household size and NYC drifts slightly downward across the period."
  },
  {
    "objectID": "mp02.html#task-4-rent-burden",
    "href": "mp02.html#task-4-rent-burden",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 4: Rent Burden",
    "text": "Task 4: Rent Burden\n\nHere we will be joining the Income and Rent tables to create suitable measure of rent burden. The ratio is scaled from 0-100 and this will show both a single metro’s trend and the highest/lowest CBSAs in the latest year.\n\n\n\nCode\nrbi_tbl &lt;- INCOME %&gt;%\n  as_cbsa5_census() %&gt;%\n  select(cbsa5, year, household_income) %&gt;%\n  inner_join(RENT %&gt;% as_cbsa5_census() %&gt;% select(cbsa5, year, monthly_rent),\n             by = c(\"cbsa5\",\"year\")) %&gt;%\n  inner_join(POPULATION %&gt;% as_cbsa5_census() %&gt;% select(cbsa5, year, population),\n             by = c(\"cbsa5\",\"year\")) %&gt;%\n  mutate(\n    rent_to_income = 12 * monthly_rent / household_income  # annualize rent\n  ) %&gt;%\n  left_join(cbsa_names, by = \"cbsa5\")\n\n#deduping \nrbi_tbl &lt;- rbi_tbl %&gt;%\n  arrange(cbsa5, year) %&gt;%\n  distinct(cbsa5, year, .keep_all = TRUE)\n\nname_canon &lt;- rbi_tbl %&gt;%\n  group_by(cbsa5) %&gt;%\n  filter(year == max(year, na.rm = TRUE)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(cbsa5, NAME_canon = NAME)\n\nrbi_tbl &lt;- rbi_tbl %&gt;%\n  select(-NAME) %&gt;%\n  left_join(name_canon, by = \"cbsa5\") %&gt;%\n  rename(NAME = NAME_canon)\n\n# Strict 0–100 scaling across ALL CBSA-years (0=min burden, 100=max) \nrti_min &lt;- min(rbi_tbl$rent_to_income, na.rm = TRUE)\nrti_max &lt;- max(rbi_tbl$rent_to_income, na.rm = TRUE)\n\nrbi_tbl &lt;- rbi_tbl %&gt;%\n  mutate(RBI_100 = 100 * (rent_to_income - rti_min) / (rti_max - rti_min))\n\n# 2009 U.S. (population-weighted) average on this 0–100 scale\nus_rti_2009 &lt;- rbi_tbl %&gt;%\n  filter(year == 2009) %&gt;%\n  summarise(avg = weighted.mean(rent_to_income, w = population, na.rm = TRUE)) %&gt;%\n  pull(avg)\nus_rbi2009_on_100 &lt;- 100 * (us_rti_2009 - rti_min) / (rti_max - rti_min)\n\nlibrary(DT)\nlibrary(htmltools)\n\n# selector for NYC\nfocus_cbsa5 &lt;- 35620L\nif (!focus_cbsa5 %in% rbi_tbl$cbsa5) {\n  fallback &lt;- cbsa_names %&gt;%\n    filter(stringr::str_detect(NAME, \"^New York\")) %&gt;%\n    left_join(POPULATION %&gt;% as_cbsa5_census() %&gt;% filter(year == 2009),\n              by = \"cbsa5\") %&gt;%\n    arrange(desc(population)) %&gt;%\n    slice(1)\n  if (nrow(fallback) &gt; 0) focus_cbsa5 &lt;- fallback$cbsa5[1]\n}\nfocus_name &lt;- cbsa_names %&gt;% filter(cbsa5 == focus_cbsa5) %&gt;% pull(NAME)\nfocus_name &lt;- if (length(focus_name) == 0) \"New York-Newark-Jersey City (NY Metro)\" else focus_name[1]\n\n#Single metro over time \nrbi_focus &lt;- rbi_tbl %&gt;%\n  filter(cbsa5 == focus_cbsa5) %&gt;%\n  arrange(year) %&gt;%\n  transmute(\n    Year = year,\n    `Median Rent (mo)` = scales::dollar(monthly_rent),\n    `Median HH Income` = scales::dollar(household_income),\n    `Rent / Income`    = scales::percent(rent_to_income, accuracy = 0.1),\n    `RBI (0–100)`      = round(RBI_100, 1)\n  )\n\n# Title above the table \nhtmltools::h4(\n  paste0(\n    \"Rent Burden over Time: \", focus_name,\n    \" (2009 U.S. avg ≈ \", round(us_rbi2009_on_100, 1), \")\"\n  )\n)\n\n\nRent Burden over Time: New York-Northern New Jersey-Long Island, NY-NJ-PA Metro Area (2009 U.S. avg ≈ 29.9)\n\n\nCode\ndatatable(\n  rbi_focus,\n  rownames = FALSE,\n  options = list(\n    pageLength = 10,  # default 10\n    lengthMenu = list(c(10, 25, 50, 100, -1), c(\"10\", \"25\", \"50\", \"100\", \"All\")),\n    scrollX = TRUE\n  )\n)\n\n\n\n\n\n\nCode\n# Highest to Lowest (latest year; default 10)\nlatest_year &lt;- max(rbi_tbl$year, na.rm = TRUE)\n\nrbi_latest_desc &lt;- rbi_tbl %&gt;%\n  filter(year == latest_year) %&gt;%\n  group_by(cbsa5) %&gt;%\n  slice_tail(n = 1) %&gt;% ungroup() %&gt;%\n  arrange(desc(RBI_100)) %&gt;%\n  transmute(\n    CBSA = NAME,\n    Year = year,\n    `RBI (0–100)` = round(RBI_100, 1),\n    `Rent / Income` = scales::percent(rent_to_income, accuracy = 0.1)\n  )\n\nhtmltools::h4(paste0(\"Highest to Lowest Rent Burden (\", latest_year, \")\"))\n\n\nHighest to Lowest Rent Burden (2023)\n\n\nCode\ndatatable(\n  rbi_latest_desc,\n  rownames = FALSE,\n  options = list(\n    pageLength = 10,\n    lengthMenu = list(c(10, 25, 50, 100, -1), c(\"10\", \"25\", \"50\", \"100\", \"All\")),\n    scrollX = TRUE\n  )\n)\n\n\n\n\n\n\nCode\n#Lowest to Highest (same data ascending; default 10)\nrbi_latest_asc &lt;- rbi_latest_desc %&gt;% arrange(`RBI (0–100)`)\n\nhtmltools::h4(paste0(\"Lowest to Highest Rent Burden (\", latest_year, \")\"))\n\n\nLowest to Highest Rent Burden (2023)\n\n\nCode\ndatatable(\n  rbi_latest_asc,\n  rownames = FALSE,\n  options = list(\n    pageLength = 10,\n    lengthMenu = list(c(10, 25, 50, 100, -1), c(\"10\", \"25\", \"50\", \"100\", \"All\")),\n    scrollX = TRUE\n  )\n)"
  },
  {
    "objectID": "mp02.html#task-5-housing-growth",
    "href": "mp02.html#task-5-housing-growth",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 5: Housing Growth",
    "text": "Task 5: Housing Growth\n\nFor this task we will be joining the Population and permits tables to create a suitable measure of housing growth. We will be creating two measures: instantaneous and rate-based. Once done so we will combine the two metrics to create a composite score. Then we will identify CBSA’s that do well and not well.\n\n\n\nCode\nlibrary(RcppRoll)\nlibrary(DT)\nlibrary(htmltools)\n\npermits_tbl &lt;- PERMITS %&gt;%\n  transmute(cbsa5 = as.integer(CBSA),\n            year,\n            permits = new_housing_units_permitted)\n\npop_tbl &lt;- POPULATION %&gt;%\n  as_cbsa5_census() %&gt;%\n  select(cbsa5, year, population)\n\n# Rolling 5-year population change\npop_5yr &lt;- pop_tbl %&gt;%\n  arrange(cbsa5, year) %&gt;%\n  group_by(cbsa5) %&gt;%\n  mutate(pop_lag5   = dplyr::lag(population, 5),\n         pop_delta5 = population - pop_lag5) %&gt;%\n  ungroup()\n\n# Keep only CBSA-years with both permits & population\nhg_base &lt;- permits_tbl %&gt;%\n  inner_join(pop_tbl,  by = c(\"cbsa5\",\"year\")) %&gt;%\n  left_join(pop_5yr %&gt;% select(cbsa5, year, pop_delta5), by = c(\"cbsa5\",\"year\")) %&gt;%\n  left_join(cbsa_names, by = \"cbsa5\") %&gt;%\n  arrange(cbsa5, year) %&gt;%\n  distinct(cbsa5, year, .keep_all = TRUE)\n\n# Define raw metrics + smooth with trailing 5-year averages\nhg_metrics &lt;- hg_base %&gt;%\n  mutate(\n    # Instantaneous intensity: permits per 1,000 residents\n    inst_raw = 1000 * permits / population,\n    # Growth-adjusted: permits per 1,000 new residents over prior 5 years\n    rate_raw = dplyr::if_else(pop_delta5 &gt; 0, 1000 * permits / pop_delta5, NA_real_)\n  ) %&gt;%\n  group_by(cbsa5) %&gt;%\n  mutate(\n    inst_5yr = RcppRoll::roll_meanr(inst_raw, n = 5, fill = NA, align = \"right\"),\n    rate_5yr = RcppRoll::roll_meanr(rate_raw, n = 5, fill = NA, align = \"right\")\n  ) %&gt;%\n  ungroup()\n\n# Standardize each smoothed metric to 0–100 across ALL CBSA-years\n\nminmax_100 &lt;- function(x){\n  fx &lt;- x[is.finite(x)]\n  if (length(fx) == 0) return(rep(NA_real_, length(x)))\n  rng &lt;- range(fx, na.rm = TRUE); den &lt;- rng[2] - rng[1]\n  if (!is.finite(den) || den == 0) return(ifelse(is.na(x), NA_real_, 50))\n  100 * (x - rng[1]) / den\n}\n\nhg_metrics &lt;- hg_metrics %&gt;%\n  mutate(\n    inst_100 = minmax_100(inst_5yr),\n    rate_100 = minmax_100(rate_5yr)\n  )\n\n# Find the latest valid year for each metric (avoid empty tables)\n\nlatest_year_inst &lt;- max(hg_metrics$year[is.finite(hg_metrics$inst_100)], na.rm = TRUE)\nlatest_year_rate &lt;- max(hg_metrics$year[is.finite(hg_metrics$rate_100)], na.rm = TRUE)\nlatest_year_comp &lt;- max(hg_metrics$year[is.finite(hg_metrics$inst_100) & is.finite(hg_metrics$rate_100)], na.rm = TRUE)\n\n# Frames for latest valid snapshots\ninst_latest &lt;- hg_metrics %&gt;%\n  filter(year == latest_year_inst, is.finite(inst_100)) %&gt;%\n  transmute(NAME, year, inst_5yr, inst_100)\n\nrate_latest &lt;- hg_metrics %&gt;%\n  filter(year == latest_year_rate, is.finite(rate_100)) %&gt;%\n  transmute(NAME, year, rate_5yr, rate_100)\n\ncomp_latest &lt;- hg_metrics %&gt;%\n  filter(year == latest_year_comp, is.finite(inst_100), is.finite(rate_100)) %&gt;%\n  mutate(composite_100 = 0.5 * inst_100 + 0.5 * rate_100) %&gt;%\n  transmute(NAME, year, inst_100, rate_100, composite_100)\n\n# How many rows to FLAG as Top/Bottom (tables still show ALL rows)\ntop_n &lt;- 15\n\n#Instantaneous (ALL CBSAs; Top/Bottom flagged)\ninst_all &lt;- inst_latest %&gt;%\n  arrange(desc(inst_100)) %&gt;%\n  mutate(\n    Rank  = row_number(),\n    Total = n(),\n    Group = dplyr::case_when(\n      Rank &lt;= top_n ~ \"Top\",\n      Rank &gt;  Total - top_n ~ \"Bottom\",\n      TRUE ~ \"Middle\"\n    ),\n    Group = factor(Group, levels = c(\"Top\",\"Middle\",\"Bottom\"))\n  ) %&gt;%\n  select(Group, Rank,\n         CBSA = NAME, Year = year,\n         `Inst (per 1k, 5-yr avg)` = inst_5yr,\n         `Index (0–100)`          = inst_100) %&gt;%\n  mutate(\n    `Inst (per 1k, 5-yr avg)` = round(`Inst (per 1k, 5-yr avg)`, 2),\n    `Index (0–100)`           = round(`Index (0–100)`, 1)\n  )\n\nhtmltools::h4(paste0(\"Instantaneous Housing Growth : All CBSAs (Top/Bottom flagged) : \", unique(inst_all$Year)))\n\n\nInstantaneous Housing Growth : All CBSAs (Top/Bottom flagged) : 2023\n\n\nCode\nDT::datatable(\n  inst_all,\n  rownames = FALSE,\n  options = list(\n    pageLength = 10,\n    lengthMenu = list(c(10, 25, 50, 100, -1), c(\"10\", \"25\", \"50\", \"100\", \"All\")),\n    scrollX = TRUE,\n    order = list(list(0, 'asc'), list(5, 'desc'))  # Group then Index\n  )\n)\n\n\n\n\n\n\nCode\n#Rate-based (ALL CBSAs; Top/Bottom flagged)\n\nrate_all &lt;- rate_latest %&gt;%\n  arrange(desc(rate_100)) %&gt;%\n  mutate(\n    Rank  = row_number(),\n    Total = n(),\n    Group = dplyr::case_when(\n      Rank &lt;= top_n ~ \"Top\",\n      Rank &gt;  Total - top_n ~ \"Bottom\",\n      TRUE ~ \"Middle\"\n    ),\n    Group = factor(Group, levels = c(\"Top\",\"Middle\",\"Bottom\"))\n  ) %&gt;%\n  select(Group, Rank,\n         CBSA = NAME, Year = year,\n         `Rate (per 1k new res., 5-yr avg)` = rate_5yr,\n         `Index (0–100)`                   = rate_100) %&gt;%\n  mutate(\n    `Rate (per 1k new res., 5-yr avg)` = round(`Rate (per 1k new res., 5-yr avg)`, 2),\n    `Index (0–100)`                    = round(`Index (0–100)`, 1)\n  )\n\nhtmltools::h4(paste0(\"Rate-based Housing Growth : All CBSAs (Top/Bottom flagged) : \", unique(rate_all$Year)))\n\n\nRate-based Housing Growth : All CBSAs (Top/Bottom flagged) : 2023\n\n\nCode\nDT::datatable(\n  rate_all,\n  rownames = FALSE,\n  options = list(\n    pageLength = 10,\n    lengthMenu = list(c(10, 25, 50, 100, -1), c(\"10\", \"25\", \"50\", \"100\", \"All\")),\n    scrollX = TRUE,\n    order = list(list(0, 'asc'), list(5, 'desc'))  # Group then Index\n  )\n)\n\n\n\n\n\n\nCode\n# 7) Composite (equal weights; ALL CBSAs; Top/Bottom flagged)\n\ncomp_all &lt;- comp_latest %&gt;%\n  arrange(desc(composite_100)) %&gt;%\n  mutate(\n    Rank  = row_number(),\n    Total = n(),\n    Group = dplyr::case_when(\n      Rank &lt;= top_n ~ \"Top\",\n      Rank &gt;  Total - top_n ~ \"Bottom\",\n      TRUE ~ \"Middle\"\n    ),\n    Group = factor(Group, levels = c(\"Top\",\"Middle\",\"Bottom\"))\n  ) %&gt;%\n  select(Group, Rank,\n         CBSA = NAME, Year = year,\n         `Instantaneous (0–100)` = inst_100,\n         `Rate-based (0–100)`    = rate_100,\n         `Composite (0–100)`     = composite_100) %&gt;%\n  mutate(\n    `Instantaneous (0–100)` = round(`Instantaneous (0–100)`, 1),\n    `Rate-based (0–100)`    = round(`Rate-based (0–100)`, 1),\n    `Composite (0–100)`     = round(`Composite (0–100)`, 1)\n  )\n\nhtmltools::h4(paste0(\"Composite Housing Growth: All CBSAs (Top/Bottom flagged) : \", unique(comp_all$Year)))\n\n\nComposite Housing Growth: All CBSAs (Top/Bottom flagged) : 2023\n\n\nCode\nDT::datatable(\n  comp_all,\n  rownames = FALSE,\n  options = list(\n    pageLength = 10,\n    lengthMenu = list(c(10, 25, 50, 100, -1), c(\"10\", \"25\", \"50\", \"100\", \"All\")),\n    scrollX = TRUE,\n    order = list(list(0, 'asc'), list(6, 'desc'))  # Group then Composite\n  )\n)"
  },
  {
    "objectID": "mp02.html#task-6visualization",
    "href": "mp02.html#task-6visualization",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 6:Visualization",
    "text": "Task 6:Visualization\n\nNow we will be creating visualizations to investigate the relationships between our Rent Burden and Housing Growth metrics."
  },
  {
    "objectID": "mp02.html#task-6-visualization",
    "href": "mp02.html#task-6-visualization",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 6: Visualization",
    "text": "Task 6: Visualization\n\nNow we will be creating visualizations to investigate the relationships between our Rent Burden and Housing Growth metrics.\n\n\n\nCode\nyears_early &lt;- 2009:2012\nyears_late  &lt;- c(2019,2020, 2021, 2022, 2023)  \n# returns NA instead of NaN if all-missing\nsafe_mean &lt;- function(x) { m &lt;- mean(x, na.rm = TRUE); if (is.nan(m)) NA_real_ else m }\n\n# Early/late rent burden and change (late - early)\nrbi_cbsa &lt;- rbi_tbl %&gt;%\n  select(cbsa5, NAME, year, RBI_100) %&gt;%\n  group_by(cbsa5, NAME) %&gt;%\n  summarise(\n    early_rbi = safe_mean(RBI_100[year %in% years_early]),\n    late_rbi  = safe_mean(RBI_100[year %in% years_late ]),\n    .groups   = \"drop\"\n  ) %&gt;%\n  mutate(delta_rbi = late_rbi - early_rbi)  # &lt;0 means burden fell (improved)\n\n# Population change over study window\npop_bounds &lt;- POPULATION %&gt;%\n  as_cbsa5_census() %&gt;%\n  select(cbsa5, year, population) %&gt;%\n  arrange(cbsa5, year) %&gt;%\n  group_by(cbsa5) %&gt;%\n  summarise(\n    pop_first = population[which.min(year)],\n    pop_last  = population[which.max(year)],\n    pop_delta = pop_last - pop_first,\n    .groups = \"drop\"\n  )\n\n# Average housing-growth composite (0–100) across period\ncomposite_by_year &lt;- hg_metrics %&gt;%\n  mutate(composite_100 = ifelse(is.finite(inst_100) & is.finite(rate_100),\n                                0.5 * inst_100 + 0.5 * rate_100, NA_real_)) %&gt;%\n  select(cbsa5, NAME, year, composite_100)\n\nhg_cbsa &lt;- composite_by_year %&gt;%\n  group_by(cbsa5, NAME) %&gt;%\n  summarise(composite_avg = safe_mean(composite_100), .groups = \"drop\")\n\n# Merge to single summary frame\nyimby_summary &lt;- rbi_cbsa %&gt;%\n  inner_join(pop_bounds, by = \"cbsa5\") %&gt;%\n  inner_join(hg_cbsa,   by = c(\"cbsa5\",\"NAME\"))\n\n# Flag “YIMBY” metros (four rules)\n\nearly_high_cut &lt;- quantile(yimby_summary$early_rbi, probs = 2/3, na.rm = TRUE)   # high early burdene\ncomp_mean      &lt;- mean(yimby_summary$composite_avg, na.rm = TRUE)                # above-average housing growth\n\nyimby_summary &lt;- yimby_summary %&gt;%\n  mutate(\n    improvement       = -delta_rbi,                      # up is better\n    cond_high_early   = early_rbi &gt;= early_high_cut,     # relatively high early burden\n    cond_burden_fell  = is.finite(delta_rbi) & delta_rbi &lt; 0,\n    cond_pop_grew     = is.finite(pop_delta)  & pop_delta &gt; 0,\n    cond_growth_above = is.finite(composite_avg) & composite_avg &gt;= comp_mean,\n    YIMBY_flag        = cond_high_early & cond_burden_fell & cond_pop_grew & cond_growth_above\n  )\n\n# Choose a few labels \nlabel_top_n &lt;- 10\nyimby_labels &lt;- yimby_summary %&gt;%\n  filter(YIMBY_flag) %&gt;%\n  arrange(desc(composite_avg)) %&gt;%\n  slice_head(n = label_top_n)\n\n#  Visual 1 — Early burden vs improvement\n\nggplot() +\n  # background: all CBSAs\n  geom_point(data = yimby_summary,\n             aes(x = early_rbi, y = improvement),\n             color = \"grey65\", alpha = 0.6, size = 1.7) +\n  # highlight: YIMBY CBSAs\n  geom_point(data = subset(yimby_summary, YIMBY_flag),\n             aes(x = early_rbi, y = improvement),\n             color = \"#E31A1C\", alpha = 0.95, size = 2.8) +\n  # label a handful of the strongest YIMBYs\n  ggrepel::geom_text_repel(\n    data = yimby_labels,\n    aes(x = early_rbi, y = improvement, label = NAME),\n    size = 3, seed = 5, max.overlaps = 30, color = \"#E31A1C\", show.legend = FALSE\n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey60\", linewidth = 0.7) +\n  geom_vline(xintercept = early_high_cut, linetype = \"dotted\", color = \"grey60\", linewidth = 0.7) +\n  labs(\n    title = \"Early Rent Burden vs. Improvement\",\n    x = \"Early Rent Burden (0–100)\",\n    y = \"Improvement in Rent Burden\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visual 2 — Housing growth vs improvement\n\nr_hg &lt;- with(yimby_summary, suppressWarnings(cor(composite_avg, improvement, use = \"complete.obs\")))\n\nggplot() +\n  geom_point(data = yimby_summary,\n             aes(x = composite_avg, y = improvement),\n             color = \"grey65\", alpha = 0.6, size = 1.7) +\n  geom_point(data = subset(yimby_summary, YIMBY_flag),\n             aes(x = composite_avg, y = improvement),\n             color = \"#1E88E5\", alpha = 0.95, size = 2.8) +\n  ggrepel::geom_text_repel(\n    data = yimby_labels,\n    aes(x = composite_avg, y = improvement, label = NAME),\n    size = 3, seed = 7, max.overlaps = 30, color = \"#1E88E5\", show.legend = FALSE\n  ) +\n  geom_smooth(data = yimby_summary,\n              aes(x = composite_avg, y = improvement),\n              method = \"lm\", se = FALSE, color = \"grey35\", linewidth = 0.9) +\n  labs(\n    title = \"Average Housing Growth vs. Rent Burden Improvement\",\n    subtitle = paste0(\"Trend line from all metros; correlation r = \",\n                      ifelse(is.finite(r_hg), round(r_hg, 2), \"NA\")),\n    x = \"Average Housing Growth Composite\",\n    y = \"Improvement in Rent Burden\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\n\nAreas like Myrtle Beach–Conway, Lakeland–Winter Haven, St. George, and Wilmington started with high rent burden, their burden fell, population grew, and housing growth was above average. On Plot 1 they sit to the right of the dotted line and above the dashed line, and on Plot 2 they cluster toward the upper-right, meaning more building lined up with improved affordability."
  },
  {
    "objectID": "mp02.html#task-7-policy-brief-its-our-homes-and-we-need-it-now",
    "href": "mp02.html#task-7-policy-brief-its-our-homes-and-we-need-it-now",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 7: Policy Brief: It’s our homes and we need it now!",
    "text": "Task 7: Policy Brief: It’s our homes and we need it now!\n\nFar too long there have been a lack of homes within the United States. We as the people need to do something about this and create legislation that will actually help create that housing for us. Who will take the stand and get done what is needed? A new bill must be proposed in order to address this issue.\nFor our policy brief we will be proposing “Its our homes and we need it now!” bill to congress. This bill will grant metros with funds where housing is permitted and built, especially where rent burdens are high. The purpose of this bill is to create incentive’s around housing being built in areas where rent burden is high. If metros can show they are building a significant amount of housing this bill will reward them with funds in which they can further re-invest into their metro area.\nThere are two sponsors for this bill. We have a sponsor from Charleston, SC and a co-sponsor from San Jose, CA. Charleston has been a YIMBY success in where it started out with high rent burden, they reduced it over time and gained above average housing growth. This is a literal success story which shows how investing in housing will cause high growth and low rent burden over time. Our co-sponsor from San Jose, CA can address how the area still have a high rent burden but a blow average housing growth. This clearly shows that they need this bill in order to create more housing growth.\nHealth Care staff and Construction workers not only build our city but also heal it. Health care workers are already in a high stress environment and do not need the burden of rent on top of that. This is a large occupation in metro areas and have stable employment in hospitals and clinics. If they were to experience a lower rent burden, then that means hospitals would be sufficiently staffed and there would be less turnover. This bill would also help construction workers because they build homes! The program would fund the jobs that these construction workers are at. It would create steady work for these unionized individuals for the years to come.\nWe are able to measure success by building metrics like rent burden index, or housing growth. Once these are built out you will need to combine them to develop a composite score. This score will be used to identify good YIMBY cities where rent burden is low and housing growth is high. So what are we waiting for! Lets pass this bill today! It’s our homes and we need it now!"
  }
]